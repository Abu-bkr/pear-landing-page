{
  "cacheItemsMap": {
    "how-we-designed-our-servers-end-to-end-for-pearai.md": {
      "document": {
        "title": "How We Designed Our Servers at PearAI",
        "date": "2024-09-01T00:00:00.000Z",
        "excerpt": "I spent the last 2 months working on PearAI, an Open-Sourced AI-Powered Code Editor. It is like having an expert on your codebase right next to you. We achieve this with Retrieval Augmented Generation. This is my new startup after finishing my B.S. & M.S from Carnegie Mellon and working for 1.5 years in High Frequency Trading as a Software Engineer. This is exactly how we designed our server.",
        "thumbnail": "/images/og-image.png",
        "tags": [
          "server",
          "server design",
          "pearai server"
        ],
        "body": {
          "raw": "\nI spent the last 2 months working on PearAI, an Open-Sourced AI-Powered Code Editor. It is like having an expert on your codebase right next to you. We achieve this with Retrieval Augmented Generation. This is my new startup after finishing my B.S. & M.S from Carnegie Mellon and working for 1.5 years in High Frequency Trading as a Software Engineer. This is exactly how we designed our server.\n\n![Pear hanging from branch](/images/blog/pear-hanging-from-branch.png)\n\nWe are launching our product next week, and we needed to build out our server this last month. As we’re building fully in public, here’s exactly how we did it. Hopefully this helps you for designing a server with scalability, resilience, and security in mind.\n\n![Pear hanging from branch](/images/blog/pearai-server-option.png)\n\nWhat’s this server for?\n\nPearAI offers two different services for LLM:\n\n1. Use PearAI’s hosted server. Pay subscription for unlimited usage. Underlying LLM is abstracted for convenience and latest AI technology.\n\n2. Use API key. Users self-manage and pay per token to the LLM Provider. On PearAI’s side, this is Open-sourced and fully transparent. Users can also use their own local LLM\n\nOur server needs the following functionalities:\n\n1. Authentication\n2. Database\n3. Proxying\n4. Observability\n5. Payment\n6. Deployment\n\n## 0. Never Start From Scratch\n\nI’m a big fan of creating my own templates and never starting from scratch again. I open-source all of these, and you can find the Flask API Template I made/used for this project here: [https://github.com/nathan-149/flask-backend-api-template](https://github.com/nathan-149/flask-backend-api-template)\n\nEdit: DONT USE FLASK, USE FASTAPI FOR ASYNC CAPABILITIES\n\n## 1. Authentication\n\nWe needed sign-up and sign-in functionality, as well as JWT tokens for each user. For this, we used Supabase, which handles authentication of users. This is how we are doing this:\n\n![Pear hanging from branch](/images/blog/pearai-authentication-flow.png)\n\nEDIT: DONT USE SUPABASE USE AUTH0\n\n## 2. Database\n\nFor our database, we had to come up with a schema for a pSQL database, including any essential data needed. This part was somewhat iterative when we were making the server, and we made changes along the way.\n\nWe used Supabase for this. I highly recommend — the API is clean with verbose docs, you can view and edit the databases in the web UI. For faster access to non-permanent memory (see Proxying), we used a Redis cache.\n\n## 3. Proxying\n\nFoundational Model API’s don’t offer a new API for each user, or have per-user quotas or rate limiting. So, we have to implement this middle layer per user. We coded up the logic of making sure the user is authenticated via the JWT, kept track of user’s requests so far, subscription status, etc. We drop all requests that are not authenticated for security and reduced load.\n\nFor things like checking / updating user’s requests so far, we use a Redis cache (which is in-memory) for lower latency, and then have a nightly cronjob to write to persistent storage on Supabase.\n\nAnother thing we wanted to implement is RabbitMQ pub/sub logic for processing requests. This will allow for more resilience to spikes in high load. This is not needed at this stage though!\n\n## 4. Observability\n\nWe needed observability and metrics on how users were using the product and why things crashed through logs. We chose OpenTelemetry/Axiom for this. This gets us dashboards for logs and usage for cheap. We just pass our logs to OpenTelemetry and use Axiom to connect to OpenTelemetry to get logs and traces in a dashboard:\n\n![Pear hanging from branch](/images/blog/axiom-dashboard.png)\n\n## 5. Payment\n\nWe needed to handle payment from users. For my last project of AI-Generated E-Books, I used Stripe, and it was OP. We used it again for this server, and it was OP again. They also handle all the payment info of customers, which is extremely convenient. Your payment flow can just redirect to Stripe’s service and payment is taken care of!\n\n## 6. Deployment\n\nI’ve only used AWS for cloud deployment. It was alright, a bit expensive. This time, we opted to use DigitalOcean after one of our contributors recommended it. DigitalOcean’s App Platform is OP and literally perfect for our server API use case. I could not recommend it more over AWS, Azure, or Google cloud. We were going to add things like Cloudflare for security, but DigitalOcean App Platform already comes with this. DigitalOcean also lets us host our Redis cache on their platform for pretty cheap (around the same as Redis’ hosting services) which is convenient.\n\n## Summary\n\nIn summary, this is the stack that we used and I would recommend:\n\n- Primary language: Python\n- API Framework: Flask\n- Authentication: Supabase\n- Payment: Stripe\n- Database: Redis + Supabase (pSQL)\n- Observability: OpenTelemetry + Axiom\n- Deployment: DigitalOcean\n\nHopefully this was helpful to someone. PearAI is open-sourced, so please help us out by starring the repo here: [https://github.com/try-pear/pearai-app](https://github.com/try-pear/pearai-app), and consider contributing! If you’d like to use the app, join the wait list here [https://trypear.ai/](https://trypear.ai/). We’re launching next week to our first batch of users!\n\nAlso, feel free to check out my youtube series on this, as I am documenting the entire startup journey with my cofounder [FryingPan](https://youtube.com/@FryingPan). [https://youtube.com/nang88](https://youtube.com/nang88). Thanks!\n",
          "html": "<p>I spent the last 2 months working on PearAI, an Open-Sourced AI-Powered Code Editor. It is like having an expert on your codebase right next to you. We achieve this with Retrieval Augmented Generation. This is my new startup after finishing my B.S. &#x26; M.S from Carnegie Mellon and working for 1.5 years in High Frequency Trading as a Software Engineer. This is exactly how we designed our server.</p>\n<p><img src=\"/images/blog/pear-hanging-from-branch.png\" alt=\"Pear hanging from branch\"></p>\n<p>We are launching our product next week, and we needed to build out our server this last month. As we’re building fully in public, here’s exactly how we did it. Hopefully this helps you for designing a server with scalability, resilience, and security in mind.</p>\n<p><img src=\"/images/blog/pearai-server-option.png\" alt=\"Pear hanging from branch\"></p>\n<p>What’s this server for?</p>\n<p>PearAI offers two different services for LLM:</p>\n<ol>\n<li>\n<p>Use PearAI’s hosted server. Pay subscription for unlimited usage. Underlying LLM is abstracted for convenience and latest AI technology.</p>\n</li>\n<li>\n<p>Use API key. Users self-manage and pay per token to the LLM Provider. On PearAI’s side, this is Open-sourced and fully transparent. Users can also use their own local LLM</p>\n</li>\n</ol>\n<p>Our server needs the following functionalities:</p>\n<ol>\n<li>Authentication</li>\n<li>Database</li>\n<li>Proxying</li>\n<li>Observability</li>\n<li>Payment</li>\n<li>Deployment</li>\n</ol>\n<h2>0. Never Start From Scratch</h2>\n<p>I’m a big fan of creating my own templates and never starting from scratch again. I open-source all of these, and you can find the Flask API Template I made/used for this project here: <a href=\"https://github.com/nathan-149/flask-backend-api-template\">https://github.com/nathan-149/flask-backend-api-template</a></p>\n<p>Edit: DONT USE FLASK, USE FASTAPI FOR ASYNC CAPABILITIES</p>\n<h2>1. Authentication</h2>\n<p>We needed sign-up and sign-in functionality, as well as JWT tokens for each user. For this, we used Supabase, which handles authentication of users. This is how we are doing this:</p>\n<p><img src=\"/images/blog/pearai-authentication-flow.png\" alt=\"Pear hanging from branch\"></p>\n<p>EDIT: DONT USE SUPABASE USE AUTH0</p>\n<h2>2. Database</h2>\n<p>For our database, we had to come up with a schema for a pSQL database, including any essential data needed. This part was somewhat iterative when we were making the server, and we made changes along the way.</p>\n<p>We used Supabase for this. I highly recommend — the API is clean with verbose docs, you can view and edit the databases in the web UI. For faster access to non-permanent memory (see Proxying), we used a Redis cache.</p>\n<h2>3. Proxying</h2>\n<p>Foundational Model API’s don’t offer a new API for each user, or have per-user quotas or rate limiting. So, we have to implement this middle layer per user. We coded up the logic of making sure the user is authenticated via the JWT, kept track of user’s requests so far, subscription status, etc. We drop all requests that are not authenticated for security and reduced load.</p>\n<p>For things like checking / updating user’s requests so far, we use a Redis cache (which is in-memory) for lower latency, and then have a nightly cronjob to write to persistent storage on Supabase.</p>\n<p>Another thing we wanted to implement is RabbitMQ pub/sub logic for processing requests. This will allow for more resilience to spikes in high load. This is not needed at this stage though!</p>\n<h2>4. Observability</h2>\n<p>We needed observability and metrics on how users were using the product and why things crashed through logs. We chose OpenTelemetry/Axiom for this. This gets us dashboards for logs and usage for cheap. We just pass our logs to OpenTelemetry and use Axiom to connect to OpenTelemetry to get logs and traces in a dashboard:</p>\n<p><img src=\"/images/blog/axiom-dashboard.png\" alt=\"Pear hanging from branch\"></p>\n<h2>5. Payment</h2>\n<p>We needed to handle payment from users. For my last project of AI-Generated E-Books, I used Stripe, and it was OP. We used it again for this server, and it was OP again. They also handle all the payment info of customers, which is extremely convenient. Your payment flow can just redirect to Stripe’s service and payment is taken care of!</p>\n<h2>6. Deployment</h2>\n<p>I’ve only used AWS for cloud deployment. It was alright, a bit expensive. This time, we opted to use DigitalOcean after one of our contributors recommended it. DigitalOcean’s App Platform is OP and literally perfect for our server API use case. I could not recommend it more over AWS, Azure, or Google cloud. We were going to add things like Cloudflare for security, but DigitalOcean App Platform already comes with this. DigitalOcean also lets us host our Redis cache on their platform for pretty cheap (around the same as Redis’ hosting services) which is convenient.</p>\n<h2>Summary</h2>\n<p>In summary, this is the stack that we used and I would recommend:</p>\n<ul>\n<li>Primary language: Python</li>\n<li>API Framework: Flask</li>\n<li>Authentication: Supabase</li>\n<li>Payment: Stripe</li>\n<li>Database: Redis + Supabase (pSQL)</li>\n<li>Observability: OpenTelemetry + Axiom</li>\n<li>Deployment: DigitalOcean</li>\n</ul>\n<p>Hopefully this was helpful to someone. PearAI is open-sourced, so please help us out by starring the repo here: <a href=\"https://github.com/try-pear/pearai-app\">https://github.com/try-pear/pearai-app</a>, and consider contributing! If you’d like to use the app, join the wait list here <a href=\"https://trypear.ai/\">https://trypear.ai/</a>. We’re launching next week to our first batch of users!</p>\n<p>Also, feel free to check out my youtube series on this, as I am documenting the entire startup journey with my cofounder <a href=\"https://youtube.com/@FryingPan\">FryingPan</a>. <a href=\"https://youtube.com/nang88\">https://youtube.com/nang88</a>. Thanks!</p>"
        },
        "_id": "how-we-designed-our-servers-end-to-end-for-pearai.md",
        "_raw": {
          "sourceFilePath": "how-we-designed-our-servers-end-to-end-for-pearai.md",
          "sourceFileName": "how-we-designed-our-servers-end-to-end-for-pearai.md",
          "sourceFileDir": ".",
          "contentType": "markdown",
          "flattenedPath": "how-we-designed-our-servers-end-to-end-for-pearai"
        },
        "type": "Post",
        "url": "/blog/how-we-designed-our-servers-end-to-end-for-pearai"
      },
      "documentHash": "1725226165280",
      "hasWarnings": false,
      "documentTypeName": "Post"
    },
    "setting-up-auto-complete-on-pearai.md": {
      "document": {
        "title": "Setting Up Auto-Complete On PearAI",
        "date": "2024-08-31T00:00:00.000Z",
        "excerpt": "PearAI supports tab autocomplete, and this is how to set this up. Tab autocomplete predicts / suggests what you would type next as you’re coding!",
        "thumbnail": "/images/og-image.png",
        "tags": [
          "autocomplete",
          "suggestions"
        ],
        "body": {
          "raw": "\nPearAI supports tab autocomplete, and this is how to set this up. Tab autocomplete predicts/suggests what you would type next as you’re coding!\n\nThis is the guide to do that:\n\n1. Setup Codestral\n\n   We recommend using Codestral, the leading model for code completion (or FIM — Fill In Middle). It’s also open-sourced! You’ll need to obtain a Codestral API key from [Mistral API](https://console.mistral.ai/).\n\n2. Add to PearAI config.json (Fill in `YOUR_API_KEY` with your API key.):\n\n   ```json\n   \"tabAutocompleteModel\": {\n      \"title\": \"Codestral\",\n      \"provider\": \"mistral\",\n      \"model\": \"codestral-latest\",\n      \"apiKey\": \"YOUR_API_KEY\"\n   }\n   ```\n\n   ![Command Palette in PearAI (Cmd/Ctrl+Shift+P)](/images/blog/open-pearai-config.png)\n\n3. Enjoy the development speed up with autocomplete!\n",
          "html": "<p>PearAI supports tab autocomplete, and this is how to set this up. Tab autocomplete predicts/suggests what you would type next as you’re coding!</p>\n<p>This is the guide to do that:</p>\n<ol>\n<li>\n<p>Setup Codestral</p>\n<p>We recommend using Codestral, the leading model for code completion (or FIM — Fill In Middle). It’s also open-sourced! You’ll need to obtain a Codestral API key from <a href=\"https://console.mistral.ai/\">Mistral API</a>.</p>\n</li>\n<li>\n<p>Add to PearAI config.json (Fill in <code>YOUR_API_KEY</code> with your API key.):</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"json\" data-theme=\"default\"><code data-language=\"json\" data-theme=\"default\"><span class=\"line\"><span style=\"color: #A5D6FF\">\"tabAutocompleteModel\"</span><span style=\"color: #C9D1D9\">: {</span></span>\n<span class=\"line\"><span style=\"color: #C9D1D9\">   </span><span style=\"color: #7EE787\">\"title\"</span><span style=\"color: #C9D1D9\">: </span><span style=\"color: #A5D6FF\">\"Codestral\"</span><span style=\"color: #C9D1D9\">,</span></span>\n<span class=\"line\"><span style=\"color: #C9D1D9\">   </span><span style=\"color: #7EE787\">\"provider\"</span><span style=\"color: #C9D1D9\">: </span><span style=\"color: #A5D6FF\">\"mistral\"</span><span style=\"color: #C9D1D9\">,</span></span>\n<span class=\"line\"><span style=\"color: #C9D1D9\">   </span><span style=\"color: #7EE787\">\"model\"</span><span style=\"color: #C9D1D9\">: </span><span style=\"color: #A5D6FF\">\"codestral-latest\"</span><span style=\"color: #C9D1D9\">,</span></span>\n<span class=\"line\"><span style=\"color: #C9D1D9\">   </span><span style=\"color: #7EE787\">\"apiKey\"</span><span style=\"color: #C9D1D9\">: </span><span style=\"color: #A5D6FF\">\"YOUR_API_KEY\"</span></span>\n<span class=\"line\"><span style=\"color: #C9D1D9\">}</span></span></code></pre></div>\n<p><img src=\"/images/blog/open-pearai-config.png\" alt=\"Command Palette in PearAI (Cmd/Ctrl+Shift+P)\"></p>\n</li>\n<li>\n<p>Enjoy the development speed up with autocomplete!</p>\n</li>\n</ol>"
        },
        "_id": "setting-up-auto-complete-on-pearai.md",
        "_raw": {
          "sourceFilePath": "setting-up-auto-complete-on-pearai.md",
          "sourceFileName": "setting-up-auto-complete-on-pearai.md",
          "sourceFileDir": ".",
          "contentType": "markdown",
          "flattenedPath": "setting-up-auto-complete-on-pearai"
        },
        "type": "Post",
        "url": "/blog/setting-up-auto-complete-on-pearai"
      },
      "documentHash": "1725226165281",
      "hasWarnings": false,
      "documentTypeName": "Post"
    }
  }
}
